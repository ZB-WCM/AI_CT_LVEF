{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Import Packages and Data Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1690823813588
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    !pip install azureml\n",
    "    !pip install -U azureml-fsspec mltable\n",
    "    !pip install azure-ai-ml\n",
    "\n",
    "import pandas as pd\n",
    "from azureml.fsspec import AzureMachineLearningFileSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1690823813955
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# long-form datastore URI format\n",
    "base_uri = f'azureml://subscriptions/{subscription}/resourcegroups/{resource_group}/workspaces/{workspace}/datastores/{datastore_name}/paths/'\n",
    "print(base_uri)\n",
    "base_uri_0 = base_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1690823817765
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "fs = AzureMachineLearningFileSystem(base_uri) # create the filesystem\n",
    "display(fs.ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'dicom_links/'\n",
    "file_uri = base_uri + file_path\n",
    "\n",
    "fs = AzureMachineLearningFileSystem(file_uri) # create the filesystem\n",
    "display(fs.ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1690823264642
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_path = ###\n",
    "file_uri = base_uri_0 + file_path\n",
    "print(file_uri)\n",
    "fs = AzureMachineLearningFileSystem(file_uri) # create the filesystem\n",
    "display(fs.ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1690823264642
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_path = ###\n",
    "file_uri = base_uri_0 + file_path\n",
    "print(file_uri)\n",
    "fs = AzureMachineLearningFileSystem(file_uri) # create the filesystem\n",
    "display(fs.ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = ###\n",
    "file_uri = base_uri + file_path\n",
    "\n",
    "fs = AzureMachineLearningFileSystem(file_uri) # create the filesystem\n",
    "display(fs.ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1690821903976
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Confirm this\n",
    "if 0:\n",
    "    import socket\n",
    "    print(socket.getaddrinfo('devcardioimagedatastore.blob.core.windows.net', 443)[0][-1][0])\n",
    "    print(socket.getaddrinfo('eus2prdimagingsharesa.blob.core.windows.net', 443)[0][-1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1690821904625
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    # Install these for regular and compressed DICOM pixel data\n",
    "    !pip install pylibjpeg pylibjpeg-libjpeg pydicom\n",
    "    !pip install python-gdcm\n",
    "    !pip install --upgrade numpy\n",
    "    !pip install pydicom\n",
    "    # Intalling pydicom may require either reinstallation or kernel restart\n",
    "    \n",
    "import pydicom\n",
    "import pandas as pd\n",
    "from azureml.fsspec import AzureMachineLearningFileSystem\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1690822186808
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# long-form datastore URI format\n",
    "base_uri = f'azureml://subscriptions/{subscription}/resourcegroups/{resource_group}/workspaces/{workspace}/datastores/{datastore_name}/paths/'\n",
    "path_uri = base_uri + ''\n",
    "print(path_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = ###\n",
    "file_uri = base_uri_0 + file_path\n",
    "print(file_uri)\n",
    "fs = AzureMachineLearningFileSystem(file_uri) # create the filesystem\n",
    "display(fs.ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnsts = [fn.split('/') for fn in fs.ls()]\n",
    "print(fnsts)\n",
    "fns = [fnst[-1] for fnst in fnsts ]\n",
    "print(fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = ###\n",
    "file_uri = base_uri_0 + file_path\n",
    "print(file_uri)\n",
    "fs = AzureMachineLearningFileSystem(file_uri) # create the filesystem\n",
    "display(fs.ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documentation: https://learn.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobserviceclient?view=azure-python \n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.blob import BlobServiceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# managed_identity_client_id should be the client id of the corresponding computing resource\n",
    "account_url = ###\n",
    "default_credential = DefaultAzureCredential(managed_identity_client_id=###)\n",
    "\n",
    "# Create the BlobServiceClient object\n",
    "blob_service_client = BlobServiceClient(account_url, credential=default_credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_name = ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine ECHO CT times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_file_name = ###\n",
    "blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_file_name)\n",
    "\n",
    "# Download the blob array file into memory (but not to local storage)\n",
    "download_stream = blob_client.download_blob()\n",
    "df_all = pd.read_json(download_stream.readall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_in_days = [i*30 for i in range(1,13)]\n",
    "interval_in_days[-1] = 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_in_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time_difference = pd.Timedelta(days=interval_in_days[0])\n",
    "max_time_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['CT_StudyDttm_x'] = pd.to_datetime(df_all['CT_StudyDttm_x'])\n",
    "df_all['ECHO_study_dttm_x'] = pd.to_datetime(df_all['ECHO_study_dttm_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['CT_StudyDttm_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['ECHO_study_dttm_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,interval in tqdm(enumerate(interval_in_days), desc=\"Processing\", unit=\"item\"):\n",
    "    max_time_difference = pd.Timedelta(days=interval)\n",
    "    filtered_df = df_all[abs(df_all['CT_StudyDttm_x'] - df_all['ECHO_study_dttm_x']) < max_time_difference]\n",
    "    display(filtered_df.shape)\n",
    "    blob_file_name = ###\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_file_name)\n",
    "    print(\"\\nUploading to Azure Storage as blob:\\n\\t\" + blob_file_name)\n",
    "    blob_client.upload_blob(filtered_df.to_json().encode(), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.columns[df_all.columns.str.startswith('CT_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.columns[df_all.columns.str.startswith('ECHO_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
